{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU  langchain_milvus python-dotenv langchain-openai langchain_ollama langchain_community GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import shutil\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders import GitLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubGPT:\n",
    "    def __init__(self):\n",
    "        self.OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.embeddings = self.__initialize_embeddings()\n",
    "        self.vector_db = self.__initialize_vector_db()\n",
    "        self.llm = self.__initialize_llm()\n",
    "        self.system_prompt = self.__initialize_system_prompt()\n",
    "\n",
    "    def __initialize_embeddings(self):\n",
    "        return OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=self.OPENAI_API_KEY\n",
    "        )\n",
    "\n",
    "    def __initialize_vector_db(self):\n",
    "        if not os.path.exists(\"./vector_db\"):\n",
    "            os.makedirs(\"./vector_db\", mode=0o777)\n",
    "            \n",
    "        return Milvus(\n",
    "            embedding_function=self.embeddings,\n",
    "            connection_args={\"uri\": \"./vector_db/milvus_example.db\"},\n",
    "            auto_id=True,\n",
    "            collection_name=\"github_gpt\",\n",
    "        )\n",
    "        \n",
    "    def __initialize_llm(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\",\n",
    "                        temperature=0.25,\n",
    "                        max_tokens=None,\n",
    "                        timeout=None,\n",
    "                        max_retries=3)\n",
    "        return llm\n",
    "    \n",
    "    def __initialize_system_prompt(self):\n",
    "        return '''\n",
    "    What are you? A well informed, intelligent chatbot which can talk to a given codebase.\n",
    "    What do you do? You are always given some file content from a codebase and a question/prompt. Your job is to generate a response.\n",
    "    What should be the tone of your output? It should be friendly, helpful, confident, narrative.\n",
    "    What outputs can we expect from you? You can be asked to genetate documentations, code, or anything else only relavant to the given codebase content.\n",
    "    '''\n",
    "        \n",
    "    @staticmethod\n",
    "    def __clean_repo_name(name):\n",
    "        return name.replace('-', '_')\n",
    "    \n",
    "    @staticmethod\n",
    "    def __declean_repo_name(name):\n",
    "        return name.replace('_', '-')\n",
    "    \n",
    "    def __add_repo_data_to_db(self):\n",
    "        data = self.loader.load()\n",
    "        print(f'Length of Data to Add: {len(data)}')\n",
    "        print(f'Adding Data to Milvus Vector DB')\n",
    "        self.vector_db.add_documents(documents=data)\n",
    "        print(f'Done Adding Data to Milvus Vector DB')\n",
    "    \n",
    "    def add_repo(self, repo_url):\n",
    "        repo_name = repo_url.split('/')[-1]\n",
    "        repo_save_path = f\"./Data/Repos\"\n",
    "        if not os.path.exists(repo_save_path):\n",
    "            os.makedirs(repo_save_path)\n",
    "        else:\n",
    "            shutil.rmtree(repo_save_path)\n",
    "            os.makedirs(repo_save_path)\n",
    "        repo_save_path = repo_save_path + \"/\" + self.__clean_repo_name(repo_name)\n",
    "        \n",
    "        print(f'Cloning the repo from: {repo_url}')\n",
    "        repo = Repo.clone_from(\n",
    "            repo_url, \n",
    "            to_path=repo_save_path,\n",
    "            branch=\"master\"\n",
    "        )\n",
    "        print(f'Repo Cloned to: {repo_save_path}')\n",
    "        self.repo_save_path = repo_save_path\n",
    "        self.branch = repo.head.reference\n",
    "        self.loader = GitLoader(repo_path=repo_save_path, branch=self.branch)\n",
    "        self.__add_repo_data_to_db()\n",
    "    \n",
    "    def load_repo(self):\n",
    "        repo_save_path = \"./Data/Repos\"\n",
    "        repo_name = os.listdir(repo_save_path)[0]\n",
    "        self.repo_save_path = repo_save_path + \"/\" + repo_name\n",
    "        self.branch = \"master\"\n",
    "        print(f'Loading repo: {repo_name}')\n",
    "        print(f'Branch: {self.branch}')\n",
    "        print(f'Repo path: {self.repo_save_path}')\n",
    "        self.loader = GitLoader(repo_path=self.repo_save_path, branch=self.branch)\n",
    "        self.__add_repo_data_to_db()\n",
    "    \n",
    "    def __retrieve_documents(self, prompt, k=3):\n",
    "        retrieved_documents = self.vector_db.similarity_search(\n",
    "            prompt,\n",
    "            k=k\n",
    "        )\n",
    "        return retrieved_documents\n",
    "    \n",
    "    @staticmethod\n",
    "    def __concatenate_documents(documents):\n",
    "        print(f'Length of docs to concatenate: {len(documents)}')\n",
    "        All_content = ''\n",
    "        for idx, doc in enumerate(documents):\n",
    "            print(f\"Retrieved Document: {idx} --- [{doc.metadata}]\")\n",
    "            All_content += \"Chunk:\" + str(idx) + \":\\n\" + doc.page_content + \"\\n\\n\"\n",
    "        print(\"\\n\\n\")\n",
    "        return All_content\n",
    "    \n",
    "    def query(self, prompt):\n",
    "        retrieved_documents = self.__retrieve_documents(prompt)\n",
    "        context = self.__concatenate_documents(retrieved_documents)\n",
    "        \n",
    "        messages = [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"{self.system_prompt}\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Context from codebase:{context}\\nUser query prompt:{prompt}\\nResponse:\\n\",\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GitHubGPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below functions / statements are responsile to \n",
    "- clone + load the data into the vectro db\n",
    "- load the already cloned data into the vector db\n",
    "Hence only uncomment one which you want to use, else the data will be already in the local vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.add_repo(\"https://github.com/SaschaNe/creatify-app\")\n",
    "# obj.load_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = obj.query(\"Explain the implementation of the processPersonProfile function in the CrispService class.\")\n",
    "print(res, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = obj.query(\"List all the middlewares which are not included in the standard Laravel 10 application and custom developed.\")\n",
    "print(res, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
